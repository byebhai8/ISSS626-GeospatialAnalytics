{
  "hash": "854eb1c036d79982b6d7fc90ec0c966e",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Take-Home Exercise 02\"\nauthor: \"Bhairavi Vairavelu\"\ndate: \"Sep 30 2024\"\ndate-modified: \"last-modified\"\nexecute:\n  eval: true\n  echo: true\n  message: false\n  freeze: true\n---\n\n\n# Discovering Impacts of COVID-19 on Thailand Tourism Economy using Spatial & Spatio-Temporal Statistics\n\n## 1.0 Overview\n\nTourism is one of Thailand’s largest industries, accounting for some 20% of the gross domestic product (GDP). In 2019, Thailand earned 90 billion US\\$ from domestic and international tourism, but the COVID-19 pandemic caused revenues to crash to 24 billion US\\$ in 2020.\n\nThe revenue from tourism industry have been recovered gradually since September 2021. However, it is important to note that the tourism economy of Thailand are not evenly distributed. Note that the tourism economy of Thailand are mainly focused on five provinces, namely Bangkok, Phuket, Chon Buri, Krabi and Chiang Mai.\n\n### 1.1 Objectives\n\nThrough this exercise, we are interested to discover the following:\n\n-   If the key indicators of tourism economy of Thailand are independent from space and space and time\n\n-   If the tourism economy is indeed spatial and spatio-temporal dependent\n\n    -   If so, we would like to detect where are the clusters and outliers, and the emerging hot spot/cold spot areas\n\n### 1.2 The Task\n\nWe will be performing the following tasks in this exercise:\n\n-   Preparation of the following Geospatial data layer:\n\n    -   Study area layer in sf polygon features (at province level incl. Bangkok)\n\n    -   Tourism economy indicators layer within the study area in sf polygon features\n\n    -   Derived tourism economy indicator layer in spacetime s3 class of sfdep, with time series kept at month and year levels\n\n-   Perform Global Spatial Autocorrelation Analysis using sfdep methods\n\n-   Perform Local Spatial Autocorrelation Analysis using sfdep methods\n\n-   Perform Emerging Hot/Cold Spot Analysis using sfdep methods\n\n### 1.3 Analytical Tools\n\nThe following R packages will be used for this exercise:\n\n-   **sf**, which is used for importing and handling geospatial data in R\n\n-   **sfdep**, which is used for spatial dependence with spatial features\n\n-   **tmap**, which is used to prepare cartographic quality choropleth maps\n\n-   **plotly**, for creating interactive graphs\n\n-   **tidyverse**, which is mainly for wrangling attribute data in R\n\n-   **lubridate**, which is used to parse and manipulate dates\n\n-   **Kendall**, which helps compute the Kendall rank correlation and Mann-Kendall trend test\n\nThe code chunk below uses p_load() of pacman package to check if the necessary packages have been installed in R. If yes, we will load the packages on R environment as shown below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npacman::p_load(sf, sfdep, tmap, plotly, tidyverse, lubridate, Kendall)\n```\n:::\n\n\n## 2.0 Data\n\n### 2.1 Getting the Data\n\nFor this exercise, we will be using two datasets:\n\n-   Thailand Domestic Tourism Statistics from Kaggle (Version 2)\n\n![](data01.png){fig-align=\"center\"}\n\n-   Thailand - Subnational Administrative Boundaries from HDX\n\n![](data02.png){fig-align=\"center\"}\n\n### 2.2 Importing the Data\n\nThese are the files we have for Thailand Domestic Tourism Statistics:\n\n![](data01_files.png){fig-align=\"center\"}\n\nNote that we will only use Version 2 of the dataset.\n\nThe code chunk below is used to load the ver2 data into our R environment.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#|eval: false\ntourism <- read_csv(\"data/aspatial/thailand_domestic_tourism_2019_2023_ver2.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 30800 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (5): province_thai, province_eng, region_thai, region_eng, variable\ndbl  (1): value\ndate (1): date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\nwrite_rds(tourism, \"data/rds/tourism.rds\")\n```\n:::\n\n\nThe code chunk below will be used to import the saved tourism.rds into R environment.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntourism <- read_rds(\"data/rds/tourism.rds\")\n```\n:::\n\n\nThese are the files we have for Thailand - Subnational Administrative Boundaries:\n\n![](data02_files.png){fig-align=\"center\"}\n\nRecall that this HDX data source contains information on 4 administrative levels - 0 for Country, 1 for Province, 2 for District and 3 for Sub-District. Hence, there were numerous files downloaded from this data source. However, we only want to focus on Province-level analysis for this exercises. As such, we will only load the ADM1 data into our R environment.\n\nThe code chunk below is used to load the ver2 data into our R environment.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#|eval: false\nboundaries = st_read(dsn = \"data/geospatial\",\n                     layer = \"tha_admbnda_adm1_rtsd_20220121\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nReading layer `tha_admbnda_adm1_rtsd_20220121' from data source \n  `C:\\byebhai8\\ISSS626-GeospatialAnalytics\\Take-Home_Ex\\Take-Home_Ex02\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 77 features and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 97.34336 ymin: 5.613038 xmax: 105.637 ymax: 20.46507\nGeodetic CRS:  WGS 84\n```\n\n\n:::\n\n```{.r .cell-code}\nwrite_rds(boundaries, \"data/rds/boundaries.rds\")\n```\n:::\n\n\nThe code chunk below will be used to import the saved boundaries.rds into R environment.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nboundaries <- read_rds(\"data/rds/boundaries.rds\")\n```\n:::\n\n\n## 3.0 Data Wrangling\n\n### 3.1 Tourism Data\n\nLet's take a quick look at the newly imported tourism data by using the glimpse() function of dplyr package as shown below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(tourism)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 30,800\nColumns: 7\n$ date          <date> 2019-01-01, 2019-01-01, 2019-01-01, 2019-01-01, 2019-01…\n$ province_thai <chr> \"กรุงเทพมหานคร\", \"ลพบุรี\", \"พระนครศรีอยุธยา\", \"สระบุรี\", \"ชัยนาท…\n$ province_eng  <chr> \"Bangkok\", \"Lopburi\", \"Phra Nakhon Si Ayutthaya\", \"Sarab…\n$ region_thai   <chr> \"ภาคกลาง\", \"ภาคกลาง\", \"ภาคกลาง\", \"ภาคกลาง\", \"ภาคกลาง\", \"…\n$ region_eng    <chr> \"central\", \"central\", \"central\", \"central\", \"central\", \"…\n$ variable      <chr> \"ratio_tourist_stay\", \"ratio_tourist_stay\", \"ratio_touri…\n$ value         <dbl> 93.37, 61.32, 73.37, 67.33, 79.31, 71.70, 64.65, 71.21, …\n```\n\n\n:::\n:::\n\n\nThe raw tourism data has 30,800 rows and 7 columns. This data will serve as the attribute table that we will use moving forward.\n\nNow, we will perform following actions using the code chunk below:\n\n-   Exclude fields that contain text in thai language - province_thai, region_thai\n\n-   Create new fields for month and year using the existing date field\n\n-   Unpivot the variable & value columns to expose new fields for our analysis\n\n-   Convert exposed fields into ratios\n\n-   Rename fields to a more appropriate name\n\n-   Keep only the columns we will use for analysis\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntourism <- tourism %>%\n  select(1,3,5,6,7) %>%\n  mutate(month = month(date, label = TRUE, abbr = TRUE),\n         year = year(date)) %>%\n  pivot_wider(names_from = variable,\n              values_from = value) %>%\n  mutate(ratio_thai_tourists = (no_tourist_thai/no_tourist_all)*100,\n         ratio_foreign_tourists = (no_tourist_foreign/no_tourist_all)*100,\n         ratio_thai_revenue = (revenue_thai/revenue_all)*100,\n         ratio_foreign_revenue = (revenue_foreign/revenue_all)*100) %>%\n  rename(province = province_eng,\n         region = region_eng) %>%\n  select(1:6, 14:17)\n```\n:::\n\n\nLet's take a look at the cleaned up tourism data set:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(tourism)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 3,850\nColumns: 10\n$ date                   <date> 2019-01-01, 2019-01-01, 2019-01-01, 2019-01-01…\n$ province               <chr> \"Bangkok\", \"Lopburi\", \"Phra Nakhon Si Ayutthaya…\n$ region                 <chr> \"central\", \"central\", \"central\", \"central\", \"ce…\n$ month                  <ord> Jan, Jan, Jan, Jan, Jan, Jan, Jan, Jan, Jan, Ja…\n$ year                   <dbl> 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019,…\n$ ratio_tourist_stay     <dbl> 93.37, 61.32, 73.37, 67.33, 79.31, 71.70, 64.65…\n$ ratio_thai_tourists    <dbl> 59.30553, 99.12046, 76.89042, 97.18389, 99.2930…\n$ ratio_foreign_tourists <dbl> 40.6944702, 0.8795373, 23.1095849, 2.8161130, 0…\n$ ratio_thai_revenue     <dbl> 36.30398, 98.81681, 73.27643, 96.66465, 99.1256…\n$ ratio_foreign_revenue  <dbl> 63.6960158, 1.1831861, 26.7235687, 3.3353460, 0…\n```\n\n\n:::\n:::\n\n\nThe updated tourism data has 3,850 rows and 10 columns. Let's analyse the fields that we have now.\n\n| S.No | Field                  | Description                                                             |\n|--------|------------------|----------------------------------------------|\n| 1    | Date                   | Day-Month-Year of when the statistic was recorded                       |\n| 2    | Province               | Name of Province in Thailand                                            |\n| 3    | Region                 | Name of Region to which the Province belongs to in Thailand             |\n| 4    | Month                  | Month of when statistic was recorded                                    |\n| 5    | Year                   | Year of when statistic was recorded                                     |\n| 6    | Ratio Tourist Stay     | Ratio of tourists who stayed overnight in the Province                  |\n| 7    | Ratio Thai Tourists    | Ratio of Thai tourists that visited the Province                        |\n| 8    | Ratio Foreign Tourists | Ratio of Foreign tourists that visited the Province                     |\n| 9    | Ratio Thai Revenue     | Ratio of revenue generated by Thai tourists who visited the Province    |\n| 10   | Ratio Foreign Revenue  | Ratio of revenue generated by Foreign tourists who visited the Province |\n\nWe can view the summary statistics of these newly exposed fields using the code chunk below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(tourism)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      date              province            region              month     \n Min.   :2019-01-01   Length:3850        Length:3850        Jan    : 385  \n 1st Qu.:2020-01-01   Class :character   Class :character   Feb    : 385  \n Median :2021-01-16   Mode  :character   Mode  :character   Mar    : 308  \n Mean   :2021-01-15                                         Apr    : 308  \n 3rd Qu.:2022-02-01                                         May    : 308  \n Max.   :2023-02-01                                         Jun    : 308  \n                                                            (Other):1848  \n      year      ratio_tourist_stay ratio_thai_tourists ratio_foreign_tourists\n Min.   :2019   Min.   : 0.00      Min.   :  0.00      Min.   :  0.00000     \n 1st Qu.:2020   1st Qu.:20.18      1st Qu.: 96.51      1st Qu.:  0.09416     \n Median :2021   Median :41.81      Median : 99.33      Median :  0.67239     \n Mean   :2021   Mean   :38.93      Mean   : 93.58      Mean   :  6.41941     \n 3rd Qu.:2022   3rd Qu.:56.20      3rd Qu.: 99.91      3rd Qu.:  3.49105     \n Max.   :2023   Max.   :95.86      Max.   :100.00      Max.   :100.00000     \n                                   NA's   :25          NA's   :25            \n ratio_thai_revenue ratio_foreign_revenue\n Min.   :  0.00     Min.   : -0.1619     \n 1st Qu.: 95.31     1st Qu.:  0.1204     \n Median : 99.12     Median :  0.8811     \n Mean   : 91.47     Mean   :  8.5333     \n 3rd Qu.: 99.88     3rd Qu.:  4.6884     \n Max.   :100.16     Max.   :100.0000     \n NA's   :24         NA's   :24           \n```\n\n\n:::\n:::\n\n\nWe can also perform exploratory data analysis using the code chunk below. By plotting histograms, we can easily identify the overall distribution of the data values.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data=tourism, \n       aes(x=`ratio_tourist_stay`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\")\n```\n\n::: {.cell-output-display}\n![](Take-Home_Ex02C_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nFrom the above plot, we can see that majority of the ratios are concentrated between 25% and 75%. The highest frequency appears around the 50% mark, indicating that a significant portion of tourists tend to stay overnight about half the time. However, this distribution tails off towards the extremes (near 100%), suggesting that fewer tourists stay overnight very frequently.\n\n### 3.2 Boundary Data\n\nLikewise, we can run the glimpse() function on the boundaries data for some quick insights.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(boundaries)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 77\nColumns: 17\n$ Shape_Leng <dbl> 2.417227, 1.695100, 1.251111, 1.884945, 3.041716, 1.739908,…\n$ Shape_Area <dbl> 0.13133873, 0.07926199, 0.05323766, 0.12698345, 0.21393797,…\n$ ADM1_EN    <chr> \"Bangkok\", \"Samut Prakan\", \"Nonthaburi\", \"Pathum Thani\", \"P…\n$ ADM1_TH    <chr> \"กรุงเทพมหานคร\", \"สมุทรปราการ\", \"นนทบุรี\", \"ปทุมธานี\", \"พระนครศรีอ…\n$ ADM1_PCODE <chr> \"TH10\", \"TH11\", \"TH12\", \"TH13\", \"TH14\", \"TH15\", \"TH16\", \"TH…\n$ ADM1_REF   <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ ADM1ALT1EN <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ ADM1ALT2EN <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ ADM1ALT1TH <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ ADM1ALT2TH <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ ADM0_EN    <chr> \"Thailand\", \"Thailand\", \"Thailand\", \"Thailand\", \"Thailand\",…\n$ ADM0_TH    <chr> \"ประเทศไทย\", \"ประเทศไทย\", \"ประเทศไทย\", \"ประเทศไทย\", \"ประเทศ…\n$ ADM0_PCODE <chr> \"TH\", \"TH\", \"TH\", \"TH\", \"TH\", \"TH\", \"TH\", \"TH\", \"TH\", \"TH\",…\n$ date       <date> 2019-02-18, 2019-02-18, 2019-02-18, 2019-02-18, 2019-02-18…\n$ validOn    <date> 2022-01-22, 2022-01-22, 2022-01-22, 2022-01-22, 2022-01-22…\n$ validTo    <date> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ geometry   <MULTIPOLYGON [°]> MULTIPOLYGON (((100.6139 13..., MULTIPOLYGON (…\n```\n\n\n:::\n:::\n\n\nThe raw boundaries data has 77 rows and 17 columns. This data contains the geospatial information we require for our analysis.\n\nNow, we will perform following actions using the code chunk below:\n\n-   Exclude fields that contain text in thai language - ADM1_TH, ADM1ALT1TH, ADM1ALT2TH, ADM0_TH\n\n-   Exclude fields that contain only one value - ADM1_REF, ADM1ALT1EN, ADM1ALT2EN, ADM0_EN, ADM0_PCODE, date, validOn, validTo\n\n-   Exclude unnecessary fields that we will not use in our analysis - Shape_Leng, Shape_Area, ADM1_PCODE\n\n-   Rename fields to a more appropriate name\n\n\n::: {.cell}\n\n```{.r .cell-code}\nboundaries <- boundaries %>%\n  select(3, 17) %>%\n  rename(province = ADM1_EN)\n```\n:::\n\n\nWe also want to keep the geometry field as a polygon instead of a multipolygon object. For this, we will first cast the geometry field into a polygon type, and then merge the polygons for each province into a single polygon.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#|eval: false\nboundary <- boundaries %>%\n  group_by(province) %>%\n  summarise(geometry = st_union(geometry))\n\nboundary$geometry <- st_cast(boundary$geometry, \"POLYGON\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in st_cast.MULTIPOLYGON(X[[i]], ...): polygon from first part only\nWarning in st_cast.MULTIPOLYGON(X[[i]], ...): polygon from first part only\nWarning in st_cast.MULTIPOLYGON(X[[i]], ...): polygon from first part only\nWarning in st_cast.MULTIPOLYGON(X[[i]], ...): polygon from first part only\nWarning in st_cast.MULTIPOLYGON(X[[i]], ...): polygon from first part only\nWarning in st_cast.MULTIPOLYGON(X[[i]], ...): polygon from first part only\nWarning in st_cast.MULTIPOLYGON(X[[i]], ...): polygon from first part only\nWarning in st_cast.MULTIPOLYGON(X[[i]], ...): polygon from first part only\nWarning in st_cast.MULTIPOLYGON(X[[i]], ...): polygon from first part only\nWarning in st_cast.MULTIPOLYGON(X[[i]], ...): polygon from first part only\nWarning in st_cast.MULTIPOLYGON(X[[i]], ...): polygon from first part only\nWarning in st_cast.MULTIPOLYGON(X[[i]], ...): polygon from first part only\nWarning in st_cast.MULTIPOLYGON(X[[i]], ...): polygon from first part only\nWarning in st_cast.MULTIPOLYGON(X[[i]], ...): polygon from first part only\nWarning in st_cast.MULTIPOLYGON(X[[i]], ...): polygon from first part only\n```\n\n\n:::\n\n```{.r .cell-code}\nboundary <- st_sf(boundary,\n                  geometry = st_geometry(boundary))\n\nwrite_rds(boundary, \"data/rds/boundary.rds\")\n```\n:::\n\n\nThe code chunk below will be used to import the saved boundary.rds into R environment.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nboundary <- read_rds(\"data/rds/boundary.rds\")\n```\n:::\n\n\nLet's take a look at the cleaned up tourism data set:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(boundary)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 77\nColumns: 2\n$ province <chr> \"Amnat Charoen\", \"Ang Thong\", \"Bangkok\", \"Bueng Kan\", \"Buri R…\n$ geometry <POLYGON [°]> POLYGON ((104.9598 16.28368..., POLYGON ((100.3329 14…\n```\n\n\n:::\n:::\n\n\nThe updated tourism data has 77 rows and 2 columns. Let's analyse the fields that we have now.\n\n| S.No | Field    | Description                                              |\n|------|----------|----------------------------------------------------------|\n| 1    | Province | Name of Province in Thailand                             |\n| 2    | Geometry | Polygon object that represents each Province in Thailand |\n\nWe can use qtm() to have a quick visual representation of the boundary data, allowing us to confirm that the geometries and province names are correct,\n\n\n::: {.cell}\n\n```{.r .cell-code}\nqtm(boundary, fill = \"province\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Number of levels of the variable \"province\" is 77, which is larger\nthan max.categories (which is 30), so levels are combined. Set\ntmap_options(max.categories = 77) in the layer function to show all levels.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nSome legend labels were too wide. These labels have been resized to 0.38, 0.36, 0.43, 0.40, 0.44, 0.44, 0.55, 0.40, 0.31, 0.29, 0.23, 0.60, 0.31, 0.39, 0.47, 0.49, 0.25, 0.50, 0.33, 0.53, 0.40, 0.29, 0.62, 0.49, 0.41, 0.58, 0.32, 0.51. Increase legend.width (argument of tm_layout) to make the legend wider and therefore the labels larger.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](Take-Home_Ex02C_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n### 3.3 Creating Time Series Cube (Month)\n\nWe will first group the tourism data by months before creating the spacetime object for monthly data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#|eval: false\ntourismMonthly <- tourism %>%\n  group_by(province, month) %>%\n  summarise(across(c(\"ratio_tourist_stay\", \"ratio_thai_tourists\", \"ratio_foreign_tourists\",\n                     \"ratio_thai_revenue\",\"ratio_foreign_revenue\"),\n                    mean,\n                    na.rm = TRUE),\n            .groups = 'drop')\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: There was 1 warning in `summarise()`.\nℹ In argument: `across(...)`.\nℹ In group 1: `province = \"Amnat Charoen\"` and `month = Jan`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))\n```\n\n\n:::\n\n```{.r .cell-code}\nwrite_rds(tourismMonthly, \"data/rds/tourismMonthly.rds\")\n```\n:::\n\n\nThe code chunk below will be used to import the saved tourismMonthly.rds into R environment.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntourismMonthly <- read_rds(\"data/rds/tourismMonthly.rds\")\n```\n:::\n\n\nWe will then create a spatio-temporal object using the spacetime() function of sfdep. We will specify the following properties:\n\n-   the data, which is the tourismMonthly data.frame object\n-   the geometry, which is the boundary sf object\n-   the location identifiers, which is the province\n-   the time column, which is month\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#|eval: false\ntourismMonthlyST <- spacetime(tourismMonthly, \n                              boundary,\n                              .loc_col = \"province\",\n                              .time_col = \"month\")\nwrite_rds(tourismMonthlyST, \"data/rds/tourismMonthlyST.rds\")\n```\n:::\n\n\nThe code chunk below will be used to import the saved tourismMonthlyST.rds into R environment.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntourismMonthlyST <- read_rds(\"data/rds/tourismMonthlyST.rds\")\n```\n:::\n\n\nWe can use is_spacetime_cube() of sfdep package to verify if tourismMonthlyST is indeed a space-time cube object.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nis_spacetime_cube(tourismMonthlyST)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n:::\n\n\nThe TRUE return confirms that tourismMonthlyST is indeed a space-time cube.\n\n### 3.4 Creating Time Series Cube (Year)\n\nSimilarly, we will group the tourism data by years before creating the spacetime object for yearly data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#|eval: false\ntourismYearly <- tourism %>%\n  group_by(province, year) %>%\n  summarise(across(c(\"ratio_tourist_stay\", \"ratio_thai_tourists\", \"ratio_foreign_tourists\",\n                     \"ratio_thai_revenue\",\"ratio_foreign_revenue\"),\n                    mean,\n                    na.rm = TRUE),\n            .groups = 'drop')\nwrite_rds(tourismYearly, \"data/rds/tourismYearly.rds\")\n```\n:::\n\n\nThe code chunk below will be used to import the saved tourismYearly.rds into R environment.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntourismYearly <- read_rds(\"data/rds/tourismYearly.rds\")\n```\n:::\n\n\nWe will then create a spatio-temporal object using the spacetime() function of sfdep. We will specify the following properties:\n\n-   the data, which is the tourismYearly data.frame object\n-   the geometry, which is the boundary sf object\n-   the location identifiers, which is the province\n-   the time column, which is year\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#|eval: false\ntourismYearlyST <- spacetime(tourismYearly, \n                              boundary,\n                              .loc_col = \"province\",\n                              .time_col = \"year\")\nwrite_rds(tourismYearlyST, \"data/rds/tourismYearlyST.rds\")\n```\n:::\n\n\nThe code chunk below will be used to import the saved tourismYearlyST.rds into R environment.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntourismYearlyST <- read_rds(\"data/rds/tourismYearlyST.rds\")\n```\n:::\n\n\nWe can use is_spacetime_cube() of sfdep package to verify if tourismYearlyST is indeed a space-time cube object.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nis_spacetime_cube(tourismYearlyST)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n:::\n\n\nThe TRUE return confirms that tourismYearlyST is indeed a space-time cube.\n\n### 3.5 Performing Relational Join\n\nWe need to combine both the geospatial data and the aspatial data into one. This will be performed using the left_join function of dplyr package. The boundary data will be used as the base data object, and the tourism data will be used as the join table.\n\nThe code chunk below is used to perform the task. The unique identifier that is used to join both data objects are province.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#|eval: false\ntourismBoundaries <- left_join(boundary, \n                               tourism, \n                               by=c(\"province\"=\"province\"))\ntourismBoundariesSF <- st_sf(tourismBoundaries,\n                             geometry = st_geometry(tourismBoundaries))\nwrite_rds(tourismBoundariesSF, \"data/rds/tourismBoundariesSF.rds\")\n```\n:::\n\n\nThe code chunk below will be used to import the saved tourismBoundariesSF.rds into R environment.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntourismBoundariesSF <- read_rds(\"data/rds/tourismBoundariesSF.rds\")\n```\n:::\n\n\nNote that no new output data has been created. Instead, the data fields from tourism data frame are now updated into the data frame of boundaries. Let's take a quick look at this joined data using the code chunk below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(tourismBoundariesSF)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 3,458\nColumns: 11\n$ province               <chr> \"Amnat Charoen\", \"Amnat Charoen\", \"Amnat Charoe…\n$ date                   <date> 2019-01-01, 2020-01-01, 2021-01-01, 2022-01-01…\n$ region                 <chr> \"east_northeast\", \"east_northeast\", \"east_north…\n$ month                  <ord> Jan, Jan, Jan, Jan, Jan, Feb, Feb, Feb, Feb, Fe…\n$ year                   <dbl> 2019, 2020, 2021, 2022, 2023, 2019, 2020, 2021,…\n$ ratio_tourist_stay     <dbl> 65.15, 58.36, 26.61, 37.36, 39.46, 63.30, 56.73…\n$ ratio_thai_tourists    <dbl> 96.24598, 96.24447, 100.00000, 99.64677, 98.148…\n$ ratio_foreign_tourists <dbl> 3.7540156, 3.7555333, 0.0000000, 0.3532321, 1.8…\n$ ratio_thai_revenue     <dbl> 94.58128, 94.76075, 100.00000, 99.03730, 96.956…\n$ ratio_foreign_revenue  <dbl> 5.4187192, 5.2392489, 0.0000000, 0.9626955, 3.0…\n$ geometry               <POLYGON [°]> POLYGON ((104.9598 16.28368..., POLYGON…\n```\n\n\n:::\n:::\n\n\nThe joined tourismBoundariesSF data has 3,458 rows and 11 columns. We can now perform exploratory data analysis using this joined data.\n\nTo have a quick look at the distribution of average ratio of tourists who stayed overnight at Thailand at Province level for the year 2023, a choropleth map will be prepared using the code chunk below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#|eval: false\ntourismBoundaries2023 <- tourismBoundariesSF %>%\n  filter(year == 2023) %>%\n  group_by(province) %>%\n  summarize(ratio_tourist_stay = mean(`ratio_tourist_stay`, na.rm = TRUE),\n            ratio_thai_tourists = mean(`ratio_thai_tourists`, na.rm = TRUE),\n            ratio_foreign_tourist = mean(`ratio_foreign_tourists`, na.rm = TRUE),\n            ratio_thai_revenue = mean(`ratio_thai_revenue`, na.rm = TRUE),\n            ratio_foreign_revenue = mean(`ratio_foreign_revenue`, na.rm = TRUE))\ntourismBoundaries2023 <- st_sf(tourismBoundaries2023,\n                             geometry = st_geometry(tourismBoundaries2023))\nwrite_rds(tourismBoundaries2023, \"data/rds/tourismBoundaries2023.rds\")\n```\n:::\n\n\nThe code chunk below will be used to import the saved tourismBoundaries2023.rds into R environment.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntourismBoundaries2023 <- read_rds(\"data/rds/tourismBoundaries2023.rds\")\nggplot(data = tourismBoundaries2023) +\n  geom_sf(aes(fill = `ratio_tourist_stay`),\n          color = NA) +\n  scale_fill_viridis_c(option = \"plasma\",\n                       name = \"Ratio Tourist Stay\") +\n  labs(title = \"Ratio of Tourists Who Stayed Overnight in 2023\",\n       subtitle = \"Average Ratio per Province\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](Take-Home_Ex02C_files/figure-html/unnamed-chunk-31-1.png){width=672}\n:::\n:::\n\n\nThe choropleth map visualizes the average ratio of tourists who stayed overnight across the difference provinces in Thailand for 2023. The color of darker purple indicates lower ratios, while bright yellow represents high ratios. This allows us to easily identify regions with varying tourist overnight stays ratios.\n\n## 4.0 Global Spatial Autocorrelation Analysis\n\nWe will now proceed to compute global spatial autocorrelation statistics and perform spatial complete randomness test for global spatial autocorrelation.\n\nThis analysis will be carried out for 2 indicators:\n-   ratio_thai_tourists\n-   ratio_foreign_tourist \n\nFirst, we need to derive the Queen's contiguity weights using the code chunk below. Note that we will be using the tourismBoundaries2023 data for this analysis.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#|eval: false\nwm_q <- tourismBoundaries2023 %>%\n  mutate(nb = st_contiguity(geometry),\n         wt = st_weights(nb, style = \"W\", allow_zero = TRUE),\n         .before = 1) \nwrite_rds(wm_q, \"data/rds/wm_q.rds\")\n```\n:::\n\n\nThe code chunk below will be used to import the saved wm_q.rds into R environment.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwm_q <- read_rds(\"data/rds/wm_q.rds\")\nset.seed(1234)\n```\n:::\n\n\n### 4.1 For ratio_thai_tourists variable\n\nIn the code chunk below, global_moran() will be used to compute the Moran's I value for the ratio_tourist_stay variable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmoranI_ratio_thai_tourists <- global_moran(wm_q$ratio_thai_tourists,\n                                           wm_q$nb,\n                                           wm_q$wt,\n                                           zero.policy = TRUE)\nglimpse(moranI_ratio_thai_tourists)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nList of 2\n $ I: num 0.187\n $ K: num 8.37\n```\n\n\n:::\n:::\n\n\nMoran's I test will be performed instead of just computing the Moran's I statistics. For this, we will be using global_moran_test() as shown in the code chunk below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglobal_moran_test(wm_q$ratio_thai_tourists,\n                  wm_q$nb,\n                  wm_q$wt,\n                  zero.policy = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tMoran I test under randomisation\n\ndata:  x  \nweights: listw  \nn reduced by no-neighbour observations  \n\nMoran I statistic standard deviate = 1.8974, p-value = 0.02889\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.154502164      -0.017857143       0.008251865 \n```\n\n\n:::\n:::\n\n\nNext, we will use Monte Carlo simulation to perform the statistical test by using global_moran_perm() as shown in the code chunk below. When we specify nsim = 99, it actually means that 100 simulations will be performed.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglobal_moran_perm(wm_q$ratio_thai_tourists,\n                  wm_q$nb,\n                  wm_q$wt,\n                  nsim = 99,\n                  zero.policy = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tMonte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.1545, observed rank = 98, p-value = 0.04\nalternative hypothesis: two.sided\n```\n\n\n:::\n:::\n\n\n### 4.2 For ratio_foreign_tourist variable\n\nLikewise, we will perform the same analysis for the ratio_foreign_tourist variable.\n\nIn the code chunk below, global_moran() will be used to compute the Moran's I value for the ratio_foreign_tourist variable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmoranI_ratio_foreign_tourist <- global_moran(wm_q$ratio_foreign_tourist,\n                                           wm_q$nb,\n                                           wm_q$wt,\n                                           zero.policy = TRUE)\nglimpse(moranI_ratio_foreign_tourist)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nList of 2\n $ I: num 0.187\n $ K: num 8.37\n```\n\n\n:::\n:::\n\n\nMoran's I test will be performed instead of just computing the Moran's I statistics. For this, we will be using global_moran_test() as shown in the code chunk below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglobal_moran_test(wm_q$ratio_foreign_tourist,\n                  wm_q$nb,\n                  wm_q$wt,\n                  zero.policy = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tMoran I test under randomisation\n\ndata:  x  \nweights: listw  \nn reduced by no-neighbour observations  \n\nMoran I statistic standard deviate = 1.8974, p-value = 0.02889\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.154502164      -0.017857143       0.008251865 \n```\n\n\n:::\n:::\n\n\nNext, we will use Monte Carlo simulation to perform the statistical test by using global_moran_perm() as shown in the code chunk below. When we specify nsim = 99, it actually means that 100 simulations will be performed.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglobal_moran_perm(wm_q$ratio_foreign_tourist,\n                  wm_q$nb,\n                  wm_q$wt,\n                  nsim = 99,\n                  zero.policy = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tMonte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.1545, observed rank = 97, p-value = 0.06\nalternative hypothesis: two.sided\n```\n\n\n:::\n:::\n\n\n### 4.3 Comparison between variables\n\nWhen we compare the Global Spatial Autocorrelation Analysis that was carried out for both the ratio_thai_tourists and ratio_foreign_tourist variables, these are the things to note.\n\nMoran's I Statistic: both variables exhibit the samem statistic of 0.196, which indicates a positive spatial autocorrelation. This suggests that regions with a high ratio of thai tourists and high ratio of foreign tourists tend to be clustered together. The standard deviates are identical as well, reflecting a similar level of significance in their spatial clustering patterns.\n\nFor the ratio of thai tourists, the Monte Carlo simulation shows a p-value of 0.04 (p < 0.05), supporting the finding of significant clustering. In contrast, the Monte Carlo p-value of 0.06 for the ratio of foreign tourists is above the conventional threshold of 0.05, indicating that the clustering is weaker for foreign tourists when compared to thai tourists.\n\n### 4.4 Summary\n\nTo conclude, this analysis indicates that both the ratio of thai and foreign tourists display significant positive spatial autocorrelation, with the ratios clustered in specific geographic areas. While the thai tourists ratio shows strong statistical significance in both tests, the evidence for clustering of foreign tourists is slightly weaker, particularly in the Monte Carlo simulation.\n\n## 5.0 Local Spatial Autocorrelation Analysis\n\nWe will now proceed to compute local spatial autocorrelation statistics and plot the LISA map for our analysis.\n\nLISA map is a categorical map that shows outliers and clusters. There are two types of outliers: high-low and low-high. Likewise, there are two types of clusters: high-high and low-low. LISA map is actually an interpreted map by combining Local Moran's I of geographical areas and their respective p-values.\n\nThis analysis will be carried out for 2 indicators:\n-   ratio_thai_revenue\n-   ratio_foreign_revenue \n\n### 5.1 For ratio_thai_revenue variable\n\nWe will compute Local Moran's I of ratio_thai_revenue at province level by using local_moran() of sfdep package.\n\n\n::: {.cell}\n\n:::\n\n\nIn the code chunk below, we will use tmap function to visualize Local Moran's I.\n\n\n::: {.cell}\n\n:::\n\n\nx\n\n## 6.0 Emerging Hot Spot Analysis\n\nEmerging Hot Spot Analysis is a spatio-temporal analysis method for revealing and describing how hot spot and cold spot areas evolve over time. This analysis consists of 4 main steps:\n\n-   Building a spacetime cube, which we've already created for both monthly and yearly tourism data\n-   Calculating local Gi* statistic for each bin by using an FDR correction\n-   Evaluating these hot and cold spot trends by using Mann-Kendall trend test\n-   Categorizing each study area location by referring to the resultant trend z-score and p-value for each province, and with the hot spot z-score and p-value for each bin\n\nMann-Kendall Test is a monotonic series or function that only increases or decreases and never changes direction. So long as the function either stays flat or continues to increase, its monotonic.\n\nH0: No monotonic trend\nH1: Monotonic trend is present\n\nWe will reject the null hypothesis if the p-value is smaller than the alpha value.\n\nWe will conduct this analysis for both our monthly and yearly tourism data.\n\n### 6.1 Monthly Analysis\n\nWe will first compute the local Gi* statistics.\n\nThe code chunk below will be used to identify neighbors and to derive an inverse distance weight.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntourismMonthlynb <- tourismMonthlyST %>%\n  activate(\"geometry\") %>%\n  mutate(nb = include_self(\n    st_contiguity(geometry)),\n    wt = st_inverse_distance(nb, \n                             geometry, \n                             scale = 1,\n                             alpha = 1),\n    .before = 1) %>%\n  set_nbs(\"nb\") %>%\n  set_wts(\"wt\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n! Polygon provided. Using point on surface.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: There was 1 warning in `stopifnot()`.\nℹ In argument: `wt = st_inverse_distance(nb, geometry, scale = 1, alpha = 1)`.\nCaused by warning in `st_point_on_surface.sfc()`:\n! st_point_on_surface may not give correct results for longitude/latitude data\n```\n\n\n:::\n:::\n\n\nNote that this dataset now has neighbors and weights for each time-slice.\n\nWe can now use these columns to manually calculate the local Gi* for each province. We can do this by grouping by month and using local_gstar_perm() of sfdep package. After which, we will use unnest() to unnest gi_star column of the newly created gi_stars data.frame.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngi_stars <- tourismMonthlynb %>% \n  group_by(month) %>% \n  mutate(gi_star = local_gstar_perm(ratio_thai_revenue, nb, wt)) %>% \n  tidyr::unnest(gi_star)\n```\n:::\n\n\nWith these Gi* measures, we can then evaluate each province for a trend using the Mann-Kendall Test.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nehsa <- gi_stars %>%\n  group_by(province) %>%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %>%\n  tidyr::unnest_wider(mk)\nhead(ehsa)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 6\n  province          tau      sl     S     D  varS\n  <chr>           <dbl>   <dbl> <dbl> <dbl> <dbl>\n1 Amnat Charoen  0.455  0.0467     30  66.0  213.\n2 Ang Thong      0.121  0.631       8  66.0  213.\n3 Bangkok       -0.182  0.451     -12  66.0  213.\n4 Bueng Kan     -0.576  0.0112    -38  66.0  213.\n5 Buriram       -0.667  0.00319   -44  66.0  213.\n6 Chachoengsao  -0.0606 0.837      -4  66.0  213.\n```\n\n\n:::\n:::\n\n\nWe can also sort to show the significant emerging hot/cold spots.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nemerging <- ehsa %>% \n  arrange(sl, abs(tau)) %>% \n  slice(1:10)\nhead(emerging)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 6\n  province             tau      sl     S     D  varS\n  <chr>              <dbl>   <dbl> <dbl> <dbl> <dbl>\n1 Loei              -0.727 0.00127   -48  66.0  213.\n2 Buriram           -0.667 0.00319   -44  66.0  213.\n3 Samut Sakhon      -0.636 0.00493   -42  66.0  213.\n4 Saraburi          -0.636 0.00493   -42  66.0  213.\n5 Khon Kaen         -0.606 0.00749   -40  66.0  213.\n6 Nakhon Ratchasima -0.606 0.00749   -40  66.0  213.\n```\n\n\n:::\n:::\n\n\nFinally, we will perform Emerging Hot Spot Analysis by using emerging_hotspot_analysis() of sfdep package. This will take a spacetime object, and the quoted name of the variable of interest for .var argument. The k argument will be used to specify the number of time lags, which is set to 1 by default. Plus, the nsim represents the number of simulations to be performed.\n\nThis analysis will be carried out for 3 indicators:\n-   ratio_tourist_stay\n-   ratio_thai_revenue\n-   ratio_foreign_revenue \n\n#### 6.1.1 For ratio_tourist_stay\n\n\n::: {.cell}\n\n```{.r .cell-code}\nehsa_ratio_tourist_stay <- emerging_hotspot_analysis(\n  x = tourismMonthlyST, \n  .var = \"ratio_tourist_stay\", \n  k = 1, \n  nsim = 99\n)\n```\n:::\n\n\nIn the code chunk below, we will use ggplot2 functions to reveal the distribution of EHSA classes as a bar chart.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = ehsa_ratio_tourist_stay,\n       aes(x = classification)) +\n  geom_bar()\n```\n\n::: {.cell-output-display}\n![](Take-Home_Ex02C_files/figure-html/unnamed-chunk-47-1.png){width=672}\n:::\n:::\n\n\nThe figure above shows that majority of provinces have an oscilating coldspot, followed with sporadic hotspot and sporadic coldspot. Noted that there are also a high number of provinces with no patter detected, and there is at least one new hotspot.\n\n#### 6.1.2 For ratio_thai_revenue\n\n\n::: {.cell}\n\n```{.r .cell-code}\nehsa_ratio_thai_revenue <- emerging_hotspot_analysis(\n  x = tourismMonthlyST, \n  .var = \"ratio_thai_revenue\", \n  k = 1, \n  nsim = 99\n)\n```\n:::\n\n\nIn the code chunk below, we will use ggplot2 functions to reveal the distribution of EHSA classes as a bar chart.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = ehsa_ratio_thai_revenue,\n       aes(x = classification)) +\n  geom_bar()\n```\n\n::: {.cell-output-display}\n![](Take-Home_Ex02C_files/figure-html/unnamed-chunk-49-1.png){width=672}\n:::\n:::\n\n\nThe figure above shows that majority of provinces have no pattern, but there are more sporadic coldspots that sporadic hotspots.\n\n#### 6.1.3 For ratio_foreign_revenue\n\n\n::: {.cell}\n\n```{.r .cell-code}\nehsa_ratio_foreign_revenue <- emerging_hotspot_analysis(\n  x = tourismMonthlyST, \n  .var = \"ratio_foreign_revenue\", \n  k = 1, \n  nsim = 99\n)\n```\n:::\n\n\nIn the code chunk below, we will use ggplot2 functions to reveal the distribution of EHSA classes as a bar chart.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = ehsa_ratio_foreign_revenue,\n       aes(x = classification)) +\n  geom_bar()\n```\n\n::: {.cell-output-display}\n![](Take-Home_Ex02C_files/figure-html/unnamed-chunk-51-1.png){width=672}\n:::\n:::\n\n\nThe figure above shows that majority of provinces have no pattern, and there are equal numbers of sporadic coldspots and sporadic hotspots. Also, there seems to be a consecutive coldspot.\n\n### 6.2 Yearly Analysis\n\nJust as we did for the monthly analysis, we will compute the local Gi* statistics for the yearly analysis.\n\nThe code chunk below will be used to identify neighbors and to derive an inverse distance weight.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntourismYearlynb <- tourismYearlyST %>%\n  activate(\"geometry\") %>%\n  mutate(nb = include_self(\n    st_contiguity(geometry)),\n    wt = st_inverse_distance(nb, \n                             geometry, \n                             scale = 1,\n                             alpha = 1),\n    .before = 1) %>%\n  set_nbs(\"nb\") %>%\n  set_wts(\"wt\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n! Polygon provided. Using point on surface.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: There was 1 warning in `stopifnot()`.\nℹ In argument: `wt = st_inverse_distance(nb, geometry, scale = 1, alpha = 1)`.\nCaused by warning in `st_point_on_surface.sfc()`:\n! st_point_on_surface may not give correct results for longitude/latitude data\n```\n\n\n:::\n:::\n\n\nNote that this dataset now has neighbors and weights for each time-slice.\n\nWe can now use these columns to manually calculate the local Gi* for each province. We can do this by grouping by month and using local_gstar_perm() of sfdep package. After which, we will use unnest() to unnest gi_star column of the newly created gi_stars data.frame.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngi_stars2 <- tourismYearlynb %>% \n  group_by(year) %>% \n  mutate(gi_star = local_gstar_perm(ratio_thai_revenue, nb, wt)) %>% \n  tidyr::unnest(gi_star)\n```\n:::\n\n\nWith these Gi* measures, we can then evaluate each province for a trend using the Mann-Kendall Test.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nehsa2 <- gi_stars2 %>%\n  group_by(province) %>%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %>%\n  tidyr::unnest_wider(mk)\nhead(ehsa2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 6\n  province         tau    sl     S     D  varS\n  <chr>          <dbl> <dbl> <dbl> <dbl> <dbl>\n1 Amnat Charoen  0.200 0.806     2    10  16.7\n2 Ang Thong      0.600 0.221     6    10  16.7\n3 Bangkok       -0.600 0.221    -6    10  16.7\n4 Bueng Kan     -0.600 0.221    -6    10  16.7\n5 Buriram       -0.200 0.806    -2    10  16.7\n6 Chachoengsao   0.200 0.806     2    10  16.7\n```\n\n\n:::\n:::\n\n\nWe can also sort to show the significant emerging hot/cold spots.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nemerging2 <- ehsa2 %>% \n  arrange(sl, abs(tau)) %>% \n  slice(1:10)\nhead(emerging2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 6\n  province        tau     sl     S     D  varS\n  <chr>         <dbl>  <dbl> <dbl> <dbl> <dbl>\n1 Phrae         1     0.0275    10    10  16.7\n2 Khon Kaen    -0.800 0.0864    -8    10  16.7\n3 Lopburi      -0.800 0.0864    -8    10  16.7\n4 Nakhon Nayok -0.800 0.0864    -8    10  16.7\n5 Surat Thani   0.800 0.0864     8    10  16.7\n6 Bangkok      -0.600 0.221     -6    10  16.7\n```\n\n\n:::\n:::\n\n\nFinally, we will perform Emerging Hot Spot Analysis by using emerging_hotspot_analysis() of sfdep package. This will take a spacetime object, and the quoted name of the variable of interest for .var argument. The k argument will be used to specify the number of time lags, which is set to 1 by default. Plus, the nsim represents the number of simulations to be performed.\n\nThis analysis will be carried out for 3 indicators:\n-   ratio_tourist_stay\n-   ratio_thai_revenue\n-   ratio_foreign_revenue \n\n#### 6.2.1 For ratio_tourist_stay\n\n\n::: {.cell}\n\n```{.r .cell-code}\nehsa_ratio_tourist_stay2 <- emerging_hotspot_analysis(\n  x = tourismYearlyST, \n  .var = \"ratio_tourist_stay\", \n  k = 1, \n  nsim = 99\n)\n```\n:::\n\n\nIn the code chunk below, we will use ggplot2 functions to reveal the distribution of EHSA classes as a bar chart.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = ehsa_ratio_tourist_stay2,\n       aes(x = classification)) +\n  geom_bar()\n```\n\n::: {.cell-output-display}\n![](Take-Home_Ex02C_files/figure-html/unnamed-chunk-57-1.png){width=672}\n:::\n:::\n\n\nThe figure above shows that majority of provinces either are a sporadic hotspot or have no pattern detected.\n\n#### 6.2.2 For ratio_thai_revenue\n\n\n::: {.cell}\n\n```{.r .cell-code}\nehsa_ratio_thai_revenue2 <- emerging_hotspot_analysis(\n  x = tourismYearlyST, \n  .var = \"ratio_thai_revenue\", \n  k = 1, \n  nsim = 99\n)\n```\n:::\n\n\nIn the code chunk below, we will use ggplot2 functions to reveal the distribution of EHSA classes as a bar chart.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = ehsa_ratio_thai_revenue2,\n       aes(x = classification)) +\n  geom_bar()\n```\n\n::: {.cell-output-display}\n![](Take-Home_Ex02C_files/figure-html/unnamed-chunk-59-1.png){width=672}\n:::\n:::\n\n\nThe figure above shows that majority of provinces have no pattern detected, and there are equal number of provinces which are either a sporadic coldspot or hotspot. Also note that there are slightly fewer new coldspots than consecutive hotspots.\n\n#### 6.2.3 For ratio_foreign_revenue\n\n\n::: {.cell}\n\n```{.r .cell-code}\nehsa_ratio_foreign_revenue2 <- emerging_hotspot_analysis(\n  x = tourismYearlyST, \n  .var = \"ratio_foreign_revenue\", \n  k = 1, \n  nsim = 99\n)\n```\n:::\n\n\nIn the code chunk below, we will use ggplot2 functions to reveal the distribution of EHSA classes as a bar chart.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = ehsa_ratio_foreign_revenue2,\n       aes(x = classification)) +\n  geom_bar()\n```\n\n::: {.cell-output-display}\n![](Take-Home_Ex02C_files/figure-html/unnamed-chunk-61-1.png){width=672}\n:::\n:::\n\n\nThe figure above shows that majority of provinces have no pattern detected, and there are equal number of provinces which are either a sporadic coldspot or hotspot. Also note that there is one consecutive coldspot.\n",
    "supporting": [
      "Take-Home_Ex02C_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}