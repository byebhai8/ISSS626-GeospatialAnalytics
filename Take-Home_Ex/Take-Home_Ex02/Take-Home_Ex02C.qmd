---
title: "Take-Home Exercise 02"
author: "Bhairavi Vairavelu"
date: "Sep 30 2024"
date-modified: "last-modified"
execute:
  eval: true
  echo: true
  message: false
  freeze: true
---

# Discovering Impacts of COVID-19 on Thailand Tourism Economy using Spatial & Spatio-Temporal Statistics

## 1.0 Overview

Tourism is one of Thailandâ€™s largest industries, accounting for some 20% of the gross domestic product (GDP). In 2019, Thailand earned 90 billion US\$ from domestic and international tourism, but the COVID-19 pandemic caused revenues to crash to 24 billion US\$ in 2020.

The revenue from tourism industry have been recovered gradually since September 2021. However, it is important to note that the tourism economy of Thailand are not evenly distributed. Note that the tourism economy of Thailand are mainly focused on five provinces, namely Bangkok, Phuket, Chon Buri, Krabi and Chiang Mai.

### 1.1 Objectives

Through this exercise, we are interested to discover the following:

-   If the key indicators of tourism economy of Thailand are independent from space and space and time

-   If the tourism economy is indeed spatial and spatio-temporal dependent

    -   If so, we would like to detect where are the clusters and outliers, and the emerging hot spot/cold spot areas

### 1.2 The Task

We will be performing the following tasks in this exercise:

-   Preparation of the following Geospatial data layer:

    -   Study area layer in sf polygon features (at province level incl. Bangkok)

    -   Tourism economy indicators layer within the study area in sf polygon features

    -   Derived tourism economy indicator layer in spacetime s3 class of sfdep, with time series kept at month and year levels

-   Perform Global Spatial Autocorrelation Analysis using sfdep methods

-   Perform Local Spatial Autocorrelation Analysis using sfdep methods

-   Perform Emerging Hot/Cold Spot Analysis using sfdep methods

### 1.3 Analytical Tools

The following R packages will be used for this exercise:

-   **sf**, which is used for importing and handling geospatial data in R

-   **sfdep**, which is used for spatial dependence with spatial features

-   **tmap**, which is used to prepare cartographic quality choropleth maps

-   **plotly**, for creating interactive graphs

-   **tidyverse**, which is mainly for wrangling attribute data in R

-   **lubridate**, which is used to parse and manipulate dates

-   **Kendall**, which helps compute the Kendall rank correlation and Mann-Kendall trend test

The code chunk below uses p_load() of pacman package to check if the necessary packages have been installed in R. If yes, we will load the packages on R environment as shown below.

```{r}
pacman::p_load(sf, sfdep, tmap, plotly, tidyverse, lubridate, Kendall)
```

## 2.0 Data

### 2.1 Getting the Data

For this exercise, we will be using two datasets:

-   Thailand Domestic Tourism Statistics from Kaggle (Version 2)

![](data01.png){fig-align="center"}

-   Thailand - Subnational Administrative Boundaries from HDX

![](data02.png){fig-align="center"}

### 2.2 Importing the Data

These are the files we have for Thailand Domestic Tourism Statistics:

![](data01_files.png){fig-align="center"}

Note that we will only use Version 2 of the dataset.

The code chunk below is used to load the ver2 data into our R environment.

```{r}
#|eval: false
tourism <- read_csv("data/aspatial/thailand_domestic_tourism_2019_2023_ver2.csv")
write_rds(tourism, "data/rds/tourism.rds")
```

The code chunk below will be used to import the saved tourism.rds into R environment.

```{r}
tourism <- read_rds("data/rds/tourism.rds")
```

These are the files we have for Thailand - Subnational Administrative Boundaries:

![](data02_files.png){fig-align="center"}

Recall that this HDX data source contains information on 4 administrative levels - 0 for Country, 1 for Province, 2 for District and 3 for Sub-District. Hence, there were numerous files downloaded from this data source. However, we only want to focus on Province-level analysis for this exercises. As such, we will only load the ADM1 data into our R environment.

The code chunk below is used to load the ver2 data into our R environment.

```{r}
#|eval: false
boundaries = st_read(dsn = "data/geospatial",
                     layer = "tha_admbnda_adm1_rtsd_20220121")
write_rds(boundaries, "data/rds/boundaries.rds")
```

The code chunk below will be used to import the saved boundaries.rds into R environment.

```{r}
boundaries <- read_rds("data/rds/boundaries.rds")
```

## 3.0 Data Wrangling

### 3.1 Tourism Data

Let's take a quick look at the newly imported tourism data by using the glimpse() function of dplyr package as shown below.

```{r}
glimpse(tourism)
```

The raw tourism data has 30,800 rows and 7 columns. This data will serve as the attribute table that we will use moving forward.

Now, we will perform following actions using the code chunk below:

-   Exclude fields that contain text in thai language - province_thai, region_thai

-   Create new fields for month and year using the existing date field

-   Unpivot the variable & value columns to expose new fields for our analysis

-   Convert exposed fields into ratios

-   Rename fields to a more appropriate name

-   Keep only the columns we will use for analysis

```{r}
tourism <- tourism %>%
  select(1,3,5,6,7) %>%
  mutate(month = month(date, label = TRUE, abbr = TRUE),
         year = year(date)) %>%
  pivot_wider(names_from = variable,
              values_from = value) %>%
  mutate(ratio_thai_tourists = (no_tourist_thai/no_tourist_all)*100,
         ratio_foreign_tourists = (no_tourist_foreign/no_tourist_all)*100,
         ratio_thai_revenue = (revenue_thai/revenue_all)*100,
         ratio_foreign_revenue = (revenue_foreign/revenue_all)*100) %>%
  rename(province = province_eng,
         region = region_eng) %>%
  select(1:6, 14:17)
```

Let's take a look at the cleaned up tourism data set:

```{r}
glimpse(tourism)
```

The updated tourism data has 3,850 rows and 10 columns. Let's analyse the fields that we have now.

| S.No | Field                  | Description                                                             |
|--------|------------------|----------------------------------------------|
| 1    | Date                   | Day-Month-Year of when the statistic was recorded                       |
| 2    | Province               | Name of Province in Thailand                                            |
| 3    | Region                 | Name of Region to which the Province belongs to in Thailand             |
| 4    | Month                  | Month of when statistic was recorded                                    |
| 5    | Year                   | Year of when statistic was recorded                                     |
| 6    | Ratio Tourist Stay     | Ratio of tourists who stayed overnight in the Province                  |
| 7    | Ratio Thai Tourists    | Ratio of Thai tourists that visited the Province                        |
| 8    | Ratio Foreign Tourists | Ratio of Foreign tourists that visited the Province                     |
| 9    | Ratio Thai Revenue     | Ratio of revenue generated by Thai tourists who visited the Province    |
| 10   | Ratio Foreign Revenue  | Ratio of revenue generated by Foreign tourists who visited the Province |

We can view the summary statistics of these newly exposed fields using the code chunk below.

```{r}
summary(tourism)
```

We can also perform exploratory data analysis using the code chunk below. By plotting histograms, we can easily identify the overall distribution of the data values.

```{r}
ggplot(data=tourism, 
       aes(x=`ratio_tourist_stay`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")
```

From the above plot, we can see that majority of the ratios are concentrated between 25% and 75%. The highest frequency appears around the 50% mark, indicating that a significant portion of tourists tend to stay overnight about half the time. However, this distribution tails off towards the extremes (near 100%), suggesting that fewer tourists stay overnight very frequently.

### 3.2 Boundary Data

Likewise, we can run the glimpse() function on the boundaries data for some quick insights.

```{r}
glimpse(boundaries)
```

The raw boundaries data has 77 rows and 17 columns. This data contains the geospatial information we require for our analysis.

Now, we will perform following actions using the code chunk below:

-   Exclude fields that contain text in thai language - ADM1_TH, ADM1ALT1TH, ADM1ALT2TH, ADM0_TH

-   Exclude fields that contain only one value - ADM1_REF, ADM1ALT1EN, ADM1ALT2EN, ADM0_EN, ADM0_PCODE, date, validOn, validTo

-   Exclude unnecessary fields that we will not use in our analysis - Shape_Leng, Shape_Area, ADM1_PCODE

-   Rename fields to a more appropriate name

```{r}
boundaries <- boundaries %>%
  select(3, 17) %>%
  rename(province = ADM1_EN)
```

We also want to keep the geometry field as a polygon instead of a multipolygon object. For this, we will first cast the geometry field into a polygon type, and then merge the polygons for each province into a single polygon.

```{r}
#|eval: false
boundary <- boundaries %>%
  group_by(province) %>%
  summarise(geometry = st_union(geometry))

boundary$geometry <- st_cast(boundary$geometry, "POLYGON")

boundary <- st_sf(boundary,
                  geometry = st_geometry(boundary))

write_rds(boundary, "data/rds/boundary.rds")
```

The code chunk below will be used to import the saved boundary.rds into R environment.

```{r}
boundary <- read_rds("data/rds/boundary.rds")
```

Let's take a look at the cleaned up tourism data set:

```{r}
glimpse(boundary)
```

The updated tourism data has 77 rows and 2 columns. Let's analyse the fields that we have now.

| S.No | Field    | Description                                              |
|------|----------|----------------------------------------------------------|
| 1    | Province | Name of Province in Thailand                             |
| 2    | Geometry | Polygon object that represents each Province in Thailand |

We can use qtm() to have a quick visual representation of the boundary data, allowing us to confirm that the geometries and province names are correct,

```{r}
qtm(boundary, fill = "province")
```

### 3.3 Creating Time Series Cube (Month)

We will first group the tourism data by months before creating the spacetime object for monthly data.

```{r}
#|eval: false
tourismMonthly <- tourism %>%
  group_by(province, month) %>%
  summarise(across(c("ratio_tourist_stay", "ratio_thai_tourists", "ratio_foreign_tourists",
                     "ratio_thai_revenue","ratio_foreign_revenue"),
                    mean,
                    na.rm = TRUE),
            .groups = 'drop')
write_rds(tourismMonthly, "data/rds/tourismMonthly.rds")
```

The code chunk below will be used to import the saved tourismMonthly.rds into R environment.

```{r}
tourismMonthly <- read_rds("data/rds/tourismMonthly.rds")
```

We will then create a spatio-temporal object using the spacetime() function of sfdep. We will specify the following properties:

-   the data, which is the tourismMonthly data.frame object
-   the geometry, which is the boundary sf object
-   the location identifiers, which is the province
-   the time column, which is month

```{r}
#|eval: false
tourismMonthlyST <- spacetime(tourismMonthly, 
                              boundary,
                              .loc_col = "province",
                              .time_col = "month")
write_rds(tourismMonthlyST, "data/rds/tourismMonthlyST.rds")
```

The code chunk below will be used to import the saved tourismMonthlyST.rds into R environment.

```{r}
tourismMonthlyST <- read_rds("data/rds/tourismMonthlyST.rds")
```

We can use is_spacetime_cube() of sfdep package to verify if tourismMonthlyST is indeed a space-time cube object.

```{r}
is_spacetime_cube(tourismMonthlyST)
```

The TRUE return confirms that tourismMonthlyST is indeed a space-time cube.

### 3.4 Creating Time Series Cube (Year)

Similarly, we will group the tourism data by years before creating the spacetime object for yearly data.

```{r}
#|eval: false
tourismYearly <- tourism %>%
  group_by(province, year) %>%
  summarise(across(c("ratio_tourist_stay", "ratio_thai_tourists", "ratio_foreign_tourists",
                     "ratio_thai_revenue","ratio_foreign_revenue"),
                    mean,
                    na.rm = TRUE),
            .groups = 'drop')
write_rds(tourismYearly, "data/rds/tourismYearly.rds")
```

The code chunk below will be used to import the saved tourismYearly.rds into R environment.

```{r}
tourismYearly <- read_rds("data/rds/tourismYearly.rds")
```

We will then create a spatio-temporal object using the spacetime() function of sfdep. We will specify the following properties:

-   the data, which is the tourismYearly data.frame object
-   the geometry, which is the boundary sf object
-   the location identifiers, which is the province
-   the time column, which is year

```{r}
#|eval: false
tourismYearlyST <- spacetime(tourismYearly, 
                              boundary,
                              .loc_col = "province",
                              .time_col = "year")
write_rds(tourismYearlyST, "data/rds/tourismYearlyST.rds")
```

The code chunk below will be used to import the saved tourismYearlyST.rds into R environment.

```{r}
tourismYearlyST <- read_rds("data/rds/tourismYearlyST.rds")
```

We can use is_spacetime_cube() of sfdep package to verify if tourismYearlyST is indeed a space-time cube object.

```{r}
is_spacetime_cube(tourismYearlyST)
```

The TRUE return confirms that tourismYearlyST is indeed a space-time cube.

### 3.5 Performing Relational Join

We need to combine both the geospatial data and the aspatial data into one. This will be performed using the left_join function of dplyr package. The boundary data will be used as the base data object, and the tourism data will be used as the join table.

The code chunk below is used to perform the task. The unique identifier that is used to join both data objects are province.

```{r}
#|eval: false
tourismBoundaries <- left_join(boundary, 
                               tourism, 
                               by=c("province"="province"))
tourismBoundariesSF <- st_sf(tourismBoundaries,
                             geometry = st_geometry(tourismBoundaries))
write_rds(tourismBoundariesSF, "data/rds/tourismBoundariesSF.rds")
```

The code chunk below will be used to import the saved tourismBoundariesSF.rds into R environment.

```{r}
tourismBoundariesSF <- read_rds("data/rds/tourismBoundariesSF.rds")
```

Note that no new output data has been created. Instead, the data fields from tourism data frame are now updated into the data frame of boundaries. Let's take a quick look at this joined data using the code chunk below.

```{r}
glimpse(tourismBoundariesSF)
```

The joined tourismBoundariesSF data has 3,458 rows and 11 columns. We can now perform exploratory data analysis using this joined data.

To have a quick look at the distribution of average ratio of tourists who stayed overnight at Thailand at Province level for the year 2023, a choropleth map will be prepared using the code chunk below.

```{r}
#|eval: false
tourismBoundaries2023 <- tourismBoundariesSF %>%
  filter(year == 2023) %>%
  group_by(province) %>%
  summarize(ratio_tourist_stay = mean(`ratio_tourist_stay`, na.rm = TRUE),
            ratio_thai_tourists = mean(`ratio_thai_tourists`, na.rm = TRUE),
            ratio_foreign_tourist = mean(`ratio_foreign_tourists`, na.rm = TRUE),
            ratio_thai_revenue = mean(`ratio_thai_revenue`, na.rm = TRUE),
            ratio_foreign_revenue = mean(`ratio_foreign_revenue`, na.rm = TRUE))
tourismBoundaries2023 <- st_sf(tourismBoundaries2023,
                             geometry = st_geometry(tourismBoundaries2023))
write_rds(tourismBoundaries2023, "data/rds/tourismBoundaries2023.rds")
```

The code chunk below will be used to import the saved tourismBoundaries2023.rds into R environment.

```{r}
tourismBoundaries2023 <- read_rds("data/rds/tourismBoundaries2023.rds")
ggplot(data = tourismBoundaries2023) +
  geom_sf(aes(fill = `ratio_tourist_stay`),
          color = NA) +
  scale_fill_viridis_c(option = "plasma",
                       name = "Ratio Tourist Stay") +
  labs(title = "Ratio of Tourists Who Stayed Overnight in 2023",
       subtitle = "Average Ratio per Province") +
  theme_minimal()
```

The choropleth map visualizes the average ratio of tourists who stayed overnight across the difference provinces in Thailand for 2023. The color of darker purple indicates lower ratios, while bright yellow represents high ratios. This allows us to easily identify regions with varying tourist overnight stays ratios.

## 4.0 Global Spatial Autocorrelation Analysis

We will now proceed to compute global spatial autocorrelation statistics and perform spatial complete randomness test for global spatial autocorrelation.

This analysis will be carried out for 2 indicators:
-   ratio_thai_tourists
-   ratio_foreign_tourist 

First, we need to derive the Queen's contiguity weights using the code chunk below. Note that we will be using the tourismBoundaries2023 data for this analysis.

```{r}
#|eval: false
wm_q <- tourismBoundaries2023 %>%
  mutate(nb = st_contiguity(geometry),
         wt = st_weights(nb, style = "W", allow_zero = TRUE),
         .before = 1) 
write_rds(wm_q, "data/rds/wm_q.rds")
```

The code chunk below will be used to import the saved wm_q.rds into R environment.

```{r}
wm_q <- read_rds("data/rds/wm_q.rds")
set.seed(1234)
```

### 4.1 For ratio_thai_tourists variable

In the code chunk below, global_moran() will be used to compute the Moran's I value for the ratio_tourist_stay variable.

```{r}
moranI_ratio_thai_tourists <- global_moran(wm_q$ratio_thai_tourists,
                                           wm_q$nb,
                                           wm_q$wt,
                                           zero.policy = TRUE)
glimpse(moranI_ratio_thai_tourists)
```

Moran's I test will be performed instead of just computing the Moran's I statistics. For this, we will be using global_moran_test() as shown in the code chunk below.

```{r}
global_moran_test(wm_q$ratio_thai_tourists,
                  wm_q$nb,
                  wm_q$wt,
                  zero.policy = TRUE)
```

Next, we will use Monte Carlo simulation to perform the statistical test by using global_moran_perm() as shown in the code chunk below. When we specify nsim = 99, it actually means that 100 simulations will be performed.

```{r}
global_moran_perm(wm_q$ratio_thai_tourists,
                  wm_q$nb,
                  wm_q$wt,
                  nsim = 99,
                  zero.policy = TRUE)
```

### 4.2 For ratio_foreign_tourist variable

Likewise, we will perform the same analysis for the ratio_foreign_tourist variable.

In the code chunk below, global_moran() will be used to compute the Moran's I value for the ratio_foreign_tourist variable.

```{r}
moranI_ratio_foreign_tourist <- global_moran(wm_q$ratio_foreign_tourist,
                                           wm_q$nb,
                                           wm_q$wt,
                                           zero.policy = TRUE)
glimpse(moranI_ratio_foreign_tourist)
```

Moran's I test will be performed instead of just computing the Moran's I statistics. For this, we will be using global_moran_test() as shown in the code chunk below.

```{r}
global_moran_test(wm_q$ratio_foreign_tourist,
                  wm_q$nb,
                  wm_q$wt,
                  zero.policy = TRUE)
```

Next, we will use Monte Carlo simulation to perform the statistical test by using global_moran_perm() as shown in the code chunk below. When we specify nsim = 99, it actually means that 100 simulations will be performed.

```{r}
global_moran_perm(wm_q$ratio_foreign_tourist,
                  wm_q$nb,
                  wm_q$wt,
                  nsim = 99,
                  zero.policy = TRUE)
```

### 4.3 Comparison between variables

When we compare the Global Spatial Autocorrelation Analysis that was carried out for both the ratio_thai_tourists and ratio_foreign_tourist variables, these are the things to note.

Moran's I Statistic: both variables exhibit the samem statistic of 0.196, which indicates a positive spatial autocorrelation. This suggests that regions with a high ratio of thai tourists and high ratio of foreign tourists tend to be clustered together. The standard deviates are identical as well, reflecting a similar level of significance in their spatial clustering patterns.

For the ratio of thai tourists, the Monte Carlo simulation shows a p-value of 0.04 (p < 0.05), supporting the finding of significant clustering. In contrast, the Monte Carlo p-value of 0.06 for the ratio of foreign tourists is above the conventional threshold of 0.05, indicating that the clustering is weaker for foreign tourists when compared to thai tourists.

### 4.4 Summary

To conclude, this analysis indicates that both the ratio of thai and foreign tourists display significant positive spatial autocorrelation, with the ratios clustered in specific geographic areas. While the thai tourists ratio shows strong statistical significance in both tests, the evidence for clustering of foreign tourists is slightly weaker, particularly in the Monte Carlo simulation.

## 5.0 Local Spatial Autocorrelation Analysis

We will now proceed to compute local spatial autocorrelation statistics and plot the LISA map for our analysis.

LISA map is a categorical map that shows outliers and clusters. There are two types of outliers: high-low and low-high. Likewise, there are two types of clusters: high-high and low-low. LISA map is actually an interpreted map by combining Local Moran's I of geographical areas and their respective p-values.

This analysis will be carried out for 2 indicators:
-   ratio_thai_revenue
-   ratio_foreign_revenue 

### 5.1 For ratio_thai_revenue variable

We will compute Local Moran's I of ratio_thai_revenue at province level by using local_moran() of sfdep package.

```{r}

```

In the code chunk below, we will use tmap function to visualize Local Moran's I.

```{r}

```

x

## 6.0 Emerging Hot Spot Analysis

Emerging Hot Spot Analysis is a spatio-temporal analysis method for revealing and describing how hot spot and cold spot areas evolve over time. This analysis consists of 4 main steps:

-   Building a spacetime cube, which we've already created for both monthly and yearly tourism data
-   Calculating local Gi* statistic for each bin by using an FDR correction
-   Evaluating these hot and cold spot trends by using Mann-Kendall trend test
-   Categorizing each study area location by referring to the resultant trend z-score and p-value for each province, and with the hot spot z-score and p-value for each bin

Mann-Kendall Test is a monotonic series or function that only increases or decreases and never changes direction. So long as the function either stays flat or continues to increase, its monotonic.

H0: No monotonic trend
H1: Monotonic trend is present

We will reject the null hypothesis if the p-value is smaller than the alpha value.

We will conduct this analysis for both our monthly and yearly tourism data.

### 6.1 Monthly Analysis

We will first compute the local Gi* statistics.

The code chunk below will be used to identify neighbors and to derive an inverse distance weight.

```{r}
tourismMonthlynb <- tourismMonthlyST %>%
  activate("geometry") %>%
  mutate(nb = include_self(
    st_contiguity(geometry)),
    wt = st_inverse_distance(nb, 
                             geometry, 
                             scale = 1,
                             alpha = 1),
    .before = 1) %>%
  set_nbs("nb") %>%
  set_wts("wt")
```

Note that this dataset now has neighbors and weights for each time-slice.

We can now use these columns to manually calculate the local Gi* for each province. We can do this by grouping by month and using local_gstar_perm() of sfdep package. After which, we will use unnest() to unnest gi_star column of the newly created gi_stars data.frame.

```{r}
gi_stars <- tourismMonthlynb %>% 
  group_by(month) %>% 
  mutate(gi_star = local_gstar_perm(ratio_thai_revenue, nb, wt)) %>% 
  tidyr::unnest(gi_star)
```

With these Gi* measures, we can then evaluate each province for a trend using the Mann-Kendall Test.

```{r}
ehsa <- gi_stars %>%
  group_by(province) %>%
  summarise(mk = list(
    unclass(
      Kendall::MannKendall(gi_star)))) %>%
  tidyr::unnest_wider(mk)
head(ehsa)
```

We can also sort to show the significant emerging hot/cold spots.

```{r}
emerging <- ehsa %>% 
  arrange(sl, abs(tau)) %>% 
  slice(1:10)
head(emerging)
```

Finally, we will perform Emerging Hot Spot Analysis by using emerging_hotspot_analysis() of sfdep package. This will take a spacetime object, and the quoted name of the variable of interest for .var argument. The k argument will be used to specify the number of time lags, which is set to 1 by default. Plus, the nsim represents the number of simulations to be performed.

This analysis will be carried out for 3 indicators:
-   ratio_tourist_stay
-   ratio_thai_revenue
-   ratio_foreign_revenue 

#### 6.1.1 For ratio_tourist_stay

```{r}
ehsa_ratio_tourist_stay <- emerging_hotspot_analysis(
  x = tourismMonthlyST, 
  .var = "ratio_tourist_stay", 
  k = 1, 
  nsim = 99
)
```

In the code chunk below, we will use ggplot2 functions to reveal the distribution of EHSA classes as a bar chart.

```{r}
ggplot(data = ehsa_ratio_tourist_stay,
       aes(x = classification)) +
  geom_bar()
```

The figure above shows that majority of provinces have an oscilating coldspot, followed with sporadic hotspot and sporadic coldspot. Noted that there are also a high number of provinces with no patter detected, and there is at least one new hotspot.

#### 6.1.2 For ratio_thai_revenue

```{r}
ehsa_ratio_thai_revenue <- emerging_hotspot_analysis(
  x = tourismMonthlyST, 
  .var = "ratio_thai_revenue", 
  k = 1, 
  nsim = 99
)
```

In the code chunk below, we will use ggplot2 functions to reveal the distribution of EHSA classes as a bar chart.

```{r}
ggplot(data = ehsa_ratio_thai_revenue,
       aes(x = classification)) +
  geom_bar()
```

The figure above shows that majority of provinces have no pattern, but there are more sporadic coldspots that sporadic hotspots.

#### 6.1.3 For ratio_foreign_revenue

```{r}
ehsa_ratio_foreign_revenue <- emerging_hotspot_analysis(
  x = tourismMonthlyST, 
  .var = "ratio_foreign_revenue", 
  k = 1, 
  nsim = 99
)
```

In the code chunk below, we will use ggplot2 functions to reveal the distribution of EHSA classes as a bar chart.

```{r}
ggplot(data = ehsa_ratio_foreign_revenue,
       aes(x = classification)) +
  geom_bar()
```

The figure above shows that majority of provinces have no pattern, and there are equal numbers of sporadic coldspots and sporadic hotspots. Also, there seems to be a consecutive coldspot.

### 6.2 Yearly Analysis

Just as we did for the monthly analysis, we will compute the local Gi* statistics for the yearly analysis.

The code chunk below will be used to identify neighbors and to derive an inverse distance weight.

```{r}
tourismYearlynb <- tourismYearlyST %>%
  activate("geometry") %>%
  mutate(nb = include_self(
    st_contiguity(geometry)),
    wt = st_inverse_distance(nb, 
                             geometry, 
                             scale = 1,
                             alpha = 1),
    .before = 1) %>%
  set_nbs("nb") %>%
  set_wts("wt")
```

Note that this dataset now has neighbors and weights for each time-slice.

We can now use these columns to manually calculate the local Gi* for each province. We can do this by grouping by month and using local_gstar_perm() of sfdep package. After which, we will use unnest() to unnest gi_star column of the newly created gi_stars data.frame.

```{r}
gi_stars2 <- tourismYearlynb %>% 
  group_by(year) %>% 
  mutate(gi_star = local_gstar_perm(ratio_thai_revenue, nb, wt)) %>% 
  tidyr::unnest(gi_star)
```

With these Gi* measures, we can then evaluate each province for a trend using the Mann-Kendall Test.

```{r}
ehsa2 <- gi_stars2 %>%
  group_by(province) %>%
  summarise(mk = list(
    unclass(
      Kendall::MannKendall(gi_star)))) %>%
  tidyr::unnest_wider(mk)
head(ehsa2)
```

We can also sort to show the significant emerging hot/cold spots.

```{r}
emerging2 <- ehsa2 %>% 
  arrange(sl, abs(tau)) %>% 
  slice(1:10)
head(emerging2)
```

Finally, we will perform Emerging Hot Spot Analysis by using emerging_hotspot_analysis() of sfdep package. This will take a spacetime object, and the quoted name of the variable of interest for .var argument. The k argument will be used to specify the number of time lags, which is set to 1 by default. Plus, the nsim represents the number of simulations to be performed.

This analysis will be carried out for 3 indicators:
-   ratio_tourist_stay
-   ratio_thai_revenue
-   ratio_foreign_revenue 

#### 6.2.1 For ratio_tourist_stay

```{r}
ehsa_ratio_tourist_stay2 <- emerging_hotspot_analysis(
  x = tourismYearlyST, 
  .var = "ratio_tourist_stay", 
  k = 1, 
  nsim = 99
)
```

In the code chunk below, we will use ggplot2 functions to reveal the distribution of EHSA classes as a bar chart.

```{r}
ggplot(data = ehsa_ratio_tourist_stay2,
       aes(x = classification)) +
  geom_bar()
```

The figure above shows that majority of provinces either are a sporadic hotspot or have no pattern detected.

#### 6.2.2 For ratio_thai_revenue

```{r}
ehsa_ratio_thai_revenue2 <- emerging_hotspot_analysis(
  x = tourismYearlyST, 
  .var = "ratio_thai_revenue", 
  k = 1, 
  nsim = 99
)
```

In the code chunk below, we will use ggplot2 functions to reveal the distribution of EHSA classes as a bar chart.

```{r}
ggplot(data = ehsa_ratio_thai_revenue2,
       aes(x = classification)) +
  geom_bar()
```

The figure above shows that majority of provinces have no pattern detected, and there are equal number of provinces which are either a sporadic coldspot or hotspot. Also note that there are slightly fewer new coldspots than consecutive hotspots.

#### 6.2.3 For ratio_foreign_revenue

```{r}
ehsa_ratio_foreign_revenue2 <- emerging_hotspot_analysis(
  x = tourismYearlyST, 
  .var = "ratio_foreign_revenue", 
  k = 1, 
  nsim = 99
)
```

In the code chunk below, we will use ggplot2 functions to reveal the distribution of EHSA classes as a bar chart.

```{r}
ggplot(data = ehsa_ratio_foreign_revenue2,
       aes(x = classification)) +
  geom_bar()
```

The figure above shows that majority of provinces have no pattern detected, and there are equal number of provinces which are either a sporadic coldspot or hotspot. Also note that there is one consecutive coldspot.
